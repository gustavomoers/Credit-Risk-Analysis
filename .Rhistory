# Kappa : 0.3842
#
# Mcnemar's Test P-Value : 8.508e-06
#
#             Sensitivity : 0.9196
#             Specificity : 0.4211
#          Pos Pred Value : 0.7994
#          Neg Pred Value : 0.6761
#              Prevalence : 0.7150
#          Detection Rate : 0.6575
#    Detection Prevalence : 0.8225
#       Balanced Accuracy : 0.6703
#                     AUC : 0.6730
########### FOURTH MODEL: RANDOM FOREST + FEATURE SELECTION ###########################
library(randomForest)
formula.new <- "credit.rating ~ account.balance + credit.purpose + previous.credit.payment.status + savings + credit.duration.months"
formula.new <- as.formula(formula.new)
modelo_rf <- randomForest( formula.new,
data = train.data,
ntree = 100, nodesize = 10)
summary(modelo_rf)
# Testing the model
rf.prediction <- predict(modelo_rf,test.data, type = "response")
# Evaluating the model
confusionMatrix(table(data = rf.prediction, reference = test.class.var), positive = '1')
# Creating ROC Curve
rf.prediction.values <- predict(modelo_rf, test.feature.vars, type = "response")
predictions <- prediction(as.numeric(rf.prediction.values), test.class.var)
par(mfrow = c(1,2))
plot.roc.curve(predictions, title.text = "ROC curve Third Model: Random Forest")
plot.pr.curve(predictions, title.text = "Precision/Recall curve")
# Accuracy : 0.795
# 95% CI : (0.7521, 0.8335)
# No Information Rate : 0.715
# P-Value [Acc > NIR] : 0.0001623
#
# Kappa : 0.469
#
# Mcnemar's Test P-Value : 0.0358876
#
#             Sensitivity : 0.8916
#             Specificity : 0.5526
#          Pos Pred Value : 0.8333
#          Neg Pred Value : 0.6702
#              Prevalence : 0.7150
#          Detection Rate : 0.6375
#    Detection Prevalence : 0.7650
#       Balanced Accuracy : 0.7221
#                     AUC : 0.7221
rf.prediction.values <- predict(modelo_rf, test.feature.vars, type = "response")
predictions <- prediction(as.numeric(rf.prediction.values), test.class.var)
par(mfrow = c(1,2))
plot.roc.curve(predictions, title.text = "ROC curve Third Model: Random Forest + FS")
plot.pr.curve(predictions, title.text = "Precision/Recall curve")
library(randomForest)
formula.new <- "credit.rating ~ account.balance + credit.purpose + previous.credit.payment.status + savings + credit.duration.months"
formula.new <- as.formula(formula.new)
modelo_rf_new <- randomForest( formula.new,
data = train.data,
ntree = 100, nodesize = 10)
summary(modelo_rf_new)
# Testing the model
rf.prediction_new <- predict(modelo_rf_new,test.data, type = "response")
# Evaluating the model
confusionMatrix(table(data = rf.prediction_new, reference = test.class.var), positive = '1')
# Creating ROC Curve
rf.prediction.values <- predict(modelo_rf_new, test.feature.vars, type = "response")
predictions <- prediction(as.numeric(rf.prediction.values), test.class.var)
par(mfrow = c(1,2))
plot.roc.curve(predictions, title.text = "ROC curve Fourth Model: Random Forest")
plot.pr.curve(predictions, title.text = "Precision/Recall curve")
# Creating the first model, without feature selection and balancing the data
library(caret)
library(ROCR)
## separate feature and class variables
test.feature.vars <- test.data[,-1]
test.class.var <- test.data[,1]
# Creating a Logistic Regression Model
formula.init <- "credit.rating ~ ."
formula.init <- as.formula(formula.init)
lr.model <- glm(formula = formula.init, data = train.data, family = "binomial")
summary(lr.model)
# Testing the model
lr.predictions <- predict(lr.model, test.data, type="response")
lr.predictions <- round(lr.predictions)
# Evaluating the model
confusionMatrix(table(data = lr.predictions, reference = test.class.var), positive = '1')
# Creating ROC Curve
lr.prediction.values <- predict(lr.model, test.feature.vars, type = "response")
predictions <- prediction(lr.prediction.values, test.class.var)
par(mfrow = c(1,2))
plot.roc.curve(predictions, title.text = "ROC curve First Model: Logistic Regression")
plot.pr.curve(predictions, title.text = "Precision/Recall curve")
# Creating the model with the most important variables
formula.new <- "credit.rating ~ account.balance + credit.purpose + previous.credit.payment.status + savings + credit.duration.months"
formula.new <- as.formula(formula.new)
lr.model.new <- glm(formula = formula.new, data = train.data, family = "binomial")
summary(lr.model.new)
# Testing the model
lr.predictions.new <- predict(lr.model.new, test.data, type = "response")
lr.predictions.new <- round(lr.predictions.new)
# Evaluating the model
confusionMatrix(table(data = lr.predictions.new, reference = test.class.var), positive = '1')
# Creating ROC Curve
lr.prediction.values <- predict(lr.model.new, test.feature.vars, type = "response")
predictions <- prediction(lr.prediction.values, test.class.var)
par(mfrow = c(1,2))
plot.roc.curve(predictions, title.text = "ROC curve Second Model: Logistic Regression + Feature Selection")
plot.pr.curve(predictions, title.text = "Precision/Recall curve")
library(randomForest)
modelo_rf <- randomForest( credit.rating ~ .,
data = train.data,
ntree = 100, nodesize = 10)
summary(modelo_rf)
# Testing the model
rf.prediction <- predict(modelo_rf,test.data, type = "response")
# Evaluating the model
confusionMatrix(table(data = rf.prediction, reference = test.class.var), positive = '1')
# Creating ROC Curve
rf.prediction.values <- predict(modelo_rf, test.feature.vars, type = "response")
predictions <- prediction(as.numeric(rf.prediction.values), test.class.var)
par(mfrow = c(1,2))
plot.roc.curve(predictions, title.text = "ROC curve Third Model: Random Forest")
plot.pr.curve(predictions, title.text = "Precision/Recall curve")
library(randomForest)
formula.new <- "credit.rating ~ account.balance + credit.purpose + previous.credit.payment.status + savings + credit.duration.months"
formula.new <- as.formula(formula.new)
modelo_rf_new <- randomForest( formula.new,
data = train.data,
ntree = 100, nodesize = 10)
summary(modelo_rf_new)
# Testing the model
rf.prediction_new <- predict(modelo_rf_new,test.data, type = "response")
# Evaluating the model
confusionMatrix(table(data = rf.prediction_new, reference = test.class.var), positive = '1')
# Creating ROC Curve
rf.prediction.values <- predict(modelo_rf_new, test.feature.vars, type = "response")
predictions <- prediction(as.numeric(rf.prediction.values), test.class.var)
par(mfrow = c(1,2))
plot.roc.curve(predictions, title.text = "ROC curve Fourth Model: Random Forest + FS")
plot.pr.curve(predictions, title.text = "Precision/Recall curve")
library(DMwR)
train.data.balanced <- SMOTE(credit.rating~., train.data)
ggplot(train.data.balanced,aes_string(x='credit.rating')) +
geom_bar()
library(ggplot2)
ggplot(credit.df,aes_string(x='credit.rating')) +
geom_bar()
formula.new <- "credit.rating ~ account.balance + credit.purpose + previous.credit.payment.status + savings + credit.duration.months"
formula.new <- as.formula(formula.new)
lr.model.balanced <- glm(formula = formula.new, data = train.data.balanced, family = "binomial")
summary(lr.model.balanced)
# Testing the model
lr.predictions.balanced <- predict(lr.model.balanced, test.data, type = "response")
lr.predictions.balanced <- round(lr.predictions.balanced)
# Evaluating the model
confusionMatrix(table(data = lr.predictions.balanced, reference = test.class.var), positive = '1')
# Creating ROC Curve
lr.prediction.values <- predict(lr.predictions.balanced, test.feature.vars, type = "response")
predictions <- prediction(lr.prediction.values, test.class.var)
par(mfrow = c(1,2))
plot.roc.curve(predictions, title.text = "ROC curve Second Model: Logistic Regression + Feature Selection")
plot.pr.curve(predictions, title.text = "Precision/Recall curve")
lr.prediction.values <- predict(lr.model.balanced, test.feature.vars, type = "response")
predictions <- prediction(lr.prediction.values, test.class.var)
par(mfrow = c(1,2))
plot.roc.curve(predictions, title.text = "ROC curve Second Model: Logistic Regression + Feature Selection")
plot.pr.curve(predictions, title.text = "Precision/Recall curve")
lr.model.balanced <- glm(credit.rating ~ ., data = train.data.balanced, family = "binomial")
summary(lr.model.balanced)
# Testing the model
lr.predictions.balanced <- predict(lr.model.balanced, test.data, type = "response")
lr.predictions.balanced <- round(lr.predictions.balanced)
# Evaluating the model
confusionMatrix(table(data = lr.predictions.balanced, reference = test.class.var), positive = '1')
modelo_rf_balanced <- randomForest( credit.rating ~ .,
data = train.data.balanced,
ntree = 100, nodesize = 10)
summary(modelo_rf_balanced)
# Testing the model
rf.prediction_new <- predict(modelo_rf_balanced,test.data, type = "response")
# Evaluating the model
confusionMatrix(table(data = rf.prediction_new, reference = test.class.var), positive = '1')
library(C50)
# Criando uma Cost Function
Cost_func <- matrix(c(0, 1.5, 1, 0), nrow = 2, dimnames = list(c("1", "2"), c("1", "2")))
modelo_v2  <- C5.0(credit.rating ~ .,
data = train.data,
trials = 100,
cost = Cost_func)
# Criando uma Cost Function
Cost_func <- matrix(c(0, 1.5, 1, 0), nrow = 2, dimnames = list(c("1", "2"), c("1", "2")))
modelo_v2  <- C5.0(credit.rating ~ .,
data = train.data,
trials = 100,
cost = Cost_func)
?C5.0
Cost_func
summary(modelo_v2)
Cost_func <- matrix(c(0, 1.5, 1, 0), nrow = 2, dimnames = list(c("0", "1"), c("1", "2")))
Cost_func
Cost_func <- matrix(c(0, 1.5, 1, 0), nrow = 2, dimnames = list(c("0", "1"), c("0", "1")))
Cost_func
modelo_v2  <- C5.0(credit.rating ~ .,
data = train.data,
trials = 100,
cost = Cost_func)
summary(modelo_v2)
print(modelo_v2)
rf.prediction_new <- predict(modelo_v2,test.data, type = "response")
# Evaluating the model
confusionMatrix(table(data = rf.prediction_new, reference = test.class.var), positive = '1')
Cost_func <- matrix(c(0, 2.5, 2, 0), nrow = 2, dimnames = list(c("0", "1"), c("0", "1")))
# Cria o modelo
modelo_v2  <- C5.0(credit.rating ~ .,
data = train.data,
trials = 100,
cost = Cost_func)
rf.prediction_new <- predict(modelo_v2,test.data, type = "response")
# Evaluating the model
confusionMatrix(table(data = rf.prediction_new, reference = test.class.var), positive = '1')
formula.new
modelo_v2  <- C5.0(formula.new,
data = train.data,
trials = 100,
cost = Cost_func)
print(modelo_v2)
# Testing the model
rf.prediction_new <- predict(modelo_v2,test.data, type = "response")
# Evaluating the model
confusionMatrix(table(data = rf.prediction_new, reference = test.class.var), positive = '1')
rf.prediction_new <- predict(modelo_v2,test.data, type = "class")
rf.prediction_new
confusionMatrix(table(data = rf.prediction_new, reference = test.class.var), positive = '1')
lr.prediction.values <- predict(modelo_v2, test.feature.vars, type = "response")
predictions <- prediction(lr.prediction.values, test.class.var)
par(mfrow = c(1,2))
plot.roc.curve(predictions, title.text = "ROC curve Second Model: Logistic Regression + Feature Selection")
plot.pr.curve(predictions, title.text = "Precision/Recall curve")
modelo_v2  <- C5.0(credit.rating ~ .,
data = train.data,
trials = 100,
cost = Cost_func)
rf.prediction_new <- predict(modelo_v2,test.data, type = "class")
# Evaluating the model
confusionMatrix(table(data = rf.prediction_new, reference = test.class.var), positive = '1')
# Creating ROC Curve
lr.prediction.values <- predict(modelo_v2, test.feature.vars, type = "response")
predictions <- prediction(lr.prediction.values, test.class.var)
par(mfrow = c(1,2))
plot.roc.curve(predictions, title.text = "ROC curve Second Model: Logistic Regression + Feature Selection")
plot.pr.curve(predictions, title.text = "Precision/Recall curve")
lr.prediction.values <- predict(modelo_v2, test.feature.vars, type = "class")
predictions <- prediction(lr.prediction.values, test.class.var)
par(mfrow = c(1,2))
plot.roc.curve(predictions, title.text = "ROC curve Second Model: Logistic Regression + Feature Selection")
plot.pr.curve(predictions, title.text = "Precision/Recall curve")
lr.prediction.values <- predict(modelo_v2, test.feature.vars, type = "class")
predictions <- prediction(as.numeric(lr.prediction.values), test.class.var)
par(mfrow = c(1,2))
plot.roc.curve(predictions, title.text = "ROC curve Second Model: Logistic Regression + Feature Selection")
plot.pr.curve(predictions, title.text = "Precision/Recall curve")
modelo_v2  <- C5.0(formula.new,
data = train.data,
trials = 100,
cost = Cost_func)
rf.prediction_new <- predict(modelo_v2,test.data, type = "class")
# Evaluating the model
confusionMatrix(table(data = rf.prediction_new, reference = test.class.var), positive = '1')
lr.prediction.values <- predict(modelo_v2, test.feature.vars, type = "class")
predictions <- prediction(as.numeric(lr.prediction.values), test.class.var)
par(mfrow = c(1,2))
plot.roc.curve(predictions, title.text = "ROC curve Second Model: Logistic Regression + Feature Selection")
plot.pr.curve(predictions, title.text = "Precision/Recall curve")
library(e1071)
modelo_svm_v1 <- svm(credit.rating ~ .,
data = train.data,
type = 'C-classification',
kernel = 'radial')
rf.prediction_new <- predict(modelo_svm_v1,test.data, type = "class")
# Evaluating the model
confusionMatrix(table(data = rf.prediction_new, reference = test.class.var), positive = '1')
lr.prediction.values <- predict(modelo_svm_v1, test.feature.vars, type = "class")
predictions <- prediction(as.numeric(lr.prediction.values), test.class.var)
par(mfrow = c(1,2))
plot.roc.curve(predictions, title.text = "ROC curve Second Model: Logistic Regression + Feature Selection")
plot.pr.curve(predictions, title.text = "Precision/Recall curve")
modelo_v1 <- C5.0(credit.rating ~ ., data = train.data)
# Agora fazemos previsões com o modelo usando dados de teste
previsoes_v1 <- predict(modelo_v1, test.feature.vars)
caret::confusionMatrix(test.class.var, previsoes_v1, positive = '1')
roc.curve(test.class.var, previsoes_v1, plotit = T, col = "red")
library(C50)
library(caret)
library(ROCR)
library(pROC)
library(ROSE)
# Agora criamos a Curva ROC para encontrar a métrica AUC, conforme indicado no manual em pdf
roc.curve(test.class.var, previsoes_v1, plotit = T, col = "red")
rose_treino <- ROSE(credit.rating ., data = train.data, seed = 1)$data
rose_treino <- ROSE(credit.rating ~ ., data = train.data, seed = 1)$data
str(train.data)
rose_treino <- ROSE(credit.rating ~ ., data = train.data, seed = 1)
rose_treino <- ROSE(formula.new, data = train.data, seed = 1)
rose_treino <- ROSE(formula.new, data = train.data)
modelo_v1 <- C5.0(credit.rating ~ ., data = train.data.balanced)
# Agora fazemos previsões com o modelo usando dados de teste
previsoes_v1 <- predict(modelo_v1, test.feature.vars)
# Criamos a Confusion Matrix e analisamos a acurácia do modelo
# O parâmetro positive = '1' indica que a classe 1 é a positiva, ou seja, indica que sim, a transação é fraudulenta
?caret::confusionMatrix
caret::confusionMatrix(test.class.var, previsoes_v1, positive = '1')
library(neuralnet)
nn <- neuralnet(credit.rating ~ .,
data = train.data, hidden = c(5, 3))
?neuralnet
nn <- neuralnet(credit.rating ~ .,
data = train.data)
nn <- neuralnet(credit.rating ~ .,
data = train.data, linear.output = FALSE)
nn <- neuralnet(formula.new,
data = train.data, linear.output = FALSE)
m <- model.matrix(
~ categorical.vars,
data = train.data
)
m
nn <- neuralnet(formula.new,
data = m, linear.output = FALSE)
m <- model.matrix(
~ categorical.vars,
data = credit.df
)
nn <- neuralnet(formula.new,
data = m, linear.output = FALSE)
m.head
head(m)
str(m)
m <- model.matrix(
data = credit.df
)
?model.matrix
m <- model.matrix(~ credit.rating ~ account.balance + credit.purpose + previous.credit.payment.status + savings + credit.duration.months,
data = credit.df
)
m <- model.matrix(~ credit.rating + account.balance + credit.purpose + previous.credit.payment.status + savings + credit.duration.months,
data = credit.df
)
head(m)
credit.df.num <- read.csv("credit_dataset.csv", header = TRUE, sep = ",")
head(credit.df)
str(credit.df.num)
nn <- neuralnet(formula.new,
data = credit.df.num, linear.output = FALSE)
previsoes_v1 <- predict(nn, test.feature.vars)
data <- read.csv("credit_dataset.csv", header = TRUE, sep = ",")
head(data)
# Normalize the data
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins,
scale = maxs - mins))
# Split the data into training and testing set
index <- sample(1:nrow(data), round(0.75 * nrow(data)))
train_ <- scaled[index,]
test_ <- scaled[-index,]
nn <- neuralnet(formula.new,
data = train_, linear.output = FALSE)
test_
test_[-1]
previsoes_v1 <- predict(nn, test_[-1])
test_[1]
caret::confusionMatrix(test_[1], previsoes_v1, positive = '1')
previsoes_v1
previsoes_v1 <- compute(nn, test_[-1])
previsoes_v1
pr.nn$net.result
previsoes_v1$net.result
?neuralnet
?neuralnet$prediction
?neuralnet.prediction
?prediction
previsoes_v1 <- neuralnet::prediction(nn, test_[-1])
previsoes_v1 <- neuralnet::compute(nn, test_[-1])
previsoes_v1
previsoes_v1
previsoes_v1$net.result
previsoes_v1 <- neuralnet::compute(nn, test_[-1])
previsoes_v1$net.result
results <- data.frame(actual = test[1], prediction = previsoes_v1$net.result)
results <- data.frame(actual = test_[1], prediction = previsoes_v1$net.result)
View(results)
ggplot(train_,aes_string(x='credit.rating')) +
geom_bar()
ggplot(train_,aes_string(x='credit.rating')) +
geom_bar()
View(train_)
View(test_)
index <- sample(1:nrow(data), round(0.75 * nrow(data)))
train_ <- scaled[index,]
test_ <- scaled[-index,]
View(test_)
index
prop.table(test_[1])
index <- sample(1:nrow(data), round(0.6 * nrow(data)))
train_ <- scaled[index,]
test_ <- scaled[-index,]
View(test_)
nn <- neuralnet(formula.new,
data = train_, linear.output = FALSE)
previsoes_v1 <- neuralnet::compute(nn, test_[-1])
results <- data.frame(actual = test_[1], prediction = previsoes_v1$net.result)
View(results)
nn <- neuralnet(formula.new,
data = train_, hidden=c(2,1), linear.output=FALSE, threshold=0.01)
plot(nn)
previsoes_v1 <- neuralnet::compute(nn, test_[-1])
previsoes_v1$net.result
results <- data.frame(actual = test_[1], prediction = previsoes_v1$net.result)
View(results)
roundedresults<-sapply(results,round,digits=0)
roundedresultsdf=data.frame(roundedresults)
attach(roundedresultsdf)
table(actual,prediction)
View(roundedresultsdf)
table(credit.ratin,prediction)
table(credit.rating,prediction)
caret::confusionMatrix(credit.rating, prediction, positive = '1')
caret::confusionMatrix(roundedresultsdf$credit.rating, roundedresultsdf$prediction, positive = '1')
as.factor(roundedresultsdf)
str(roundedresultsdf)
roundedresultsdf = as.factor(roundedresultsdf)
str(roundedresultsdf)
caret::confusionMatrix(roundedresultsdf$credit.rating, roundedresultsdf$prediction, positive = '1')
roundedresults<-sapply(results,round,digits=0)
roundedresultsdf=data.frame(roundedresults)
attach(roundedresultsdf)
table(credit.rating,prediction)
roundedresultsdf = to.factors(roundedresultsdf, c('credit.rating','prediction'))
caret::confusionMatrix(roundedresultsdf$credit.rating, roundedresultsdf$prediction, positive = '1')
predictions <- prediction(roundedresultsdf$prediction, roundedresultsdf$credit.rating)
roundedresultsdf
train_rose = ROSE(credit.rating, data=train_)
train_rose = ROSE(credit.rating~., data=train_)
train_rose
nn <- neuralnet(credit.rating ~.,
data = train_, hidden=c(2,1), linear.output=FALSE, threshold=0.01)
previsoes_v1 <- neuralnet::compute(nn, test_[-1])
previsoes_v1$net.result
results <- data.frame(actual = test_[1], prediction = previsoes_v1$net.result)
roundedresults<-sapply(results,round,digits=0)
roundedresultsdf=data.frame(roundedresults)
attach(roundedresultsdf)
table(credit.rating,prediction)
roundedresultsdf = to.factors(roundedresultsdf, c('credit.rating','prediction'))
caret::confusionMatrix(roundedresultsdf$credit.rating, roundedresultsdf$prediction, positive = '1')
nn_1 <- neuralnet(formula.new,
data = train_rose, hidden=c(2,1), linear.output=FALSE, threshold=0.01)
nn_1 <- neuralnet(formula.new,
data = train_rose$data, hidden=c(2,1), linear.output=FALSE, threshold=0.01)
# Agora fazemos previsões com o modelo usando dados de teste
previsoes_v2 <- neuralnet::compute(nn_1, test_[-1])
results <- data.frame(actual = test_[1], prediction = previsoes_v2$net.result)
roundedresults<-sapply(results,round,digits=0)
roundedresultsdf=data.frame(roundedresults)
attach(roundedresultsdf)
table(credit.rating,prediction)
roundedresultsdf = to.factors(roundedresultsdf, c('credit.rating','prediction'))
caret::confusionMatrix(roundedresultsdf$credit.rating, roundedresultsdf$prediction, positive = '1')
nn_1 <- neuralnet(credit.rating~.,
data = train_rose$data, hidden=c(2,1), linear.output=FALSE, threshold=0.01)
# Agora fazemos previsões com o modelo usando dados de teste
previsoes_v2 <- neuralnet::compute(nn_1, test_[-1])
results <- data.frame(actual = test_[1], prediction = previsoes_v2$net.result)
roundedresults<-sapply(results,round,digits=0)
roundedresultsdf=data.frame(roundedresults)
attach(roundedresultsdf)
table(credit.rating,prediction)
roundedresultsdf = to.factors(roundedresultsdf, c('credit.rating','prediction'))
caret::confusionMatrix(roundedresultsdf$credit.rating, roundedresultsdf$prediction, positive = '1')
roundedresultsdf$prediction
predictions <- prediction(roundedresultsdf$prediction, roundedresultsdf$credit.rating)
predictions <- prediction(as.numeric(roundedresultsdf$prediction),
as.numeric(roundedresultsdf$credit.rating))
predictions <- prediction(as.numeric(prediction),
as.numeric(credit.rating))
pred = as.numeric(prediction)
actual = as.numeric(credit.rating)
predictions <- prediction(pred,actual)
lr.prediction.values
test.class.var
as.numeric(lr.prediction.values)
nn <- neuralnet(credit.rating ~.,
data = train_, hidden=c(3,2), linear.output=FALSE, threshold=0.01)
nn <- neuralnet(credit.rating ~.,
data = train_, hidden=c(2,1), linear.output=FALSE, threshold=0.01)
nn$result.matrix
plot(nn)
# Agora fazemos previsões com o modelo usando dados de teste
previsoes_v1 <- neuralnet::compute(nn, test_[-1])
previsoes_v1$net.result
results <- data.frame(actual = test_[1], prediction = previsoes_v1$net.result)
roundedresults<-sapply(results,round,digits=0)
roundedresultsdf=data.frame(roundedresults)
attach(roundedresultsdf)
table(credit.rating,prediction)
roundedresultsdf = to.factors(roundedresultsdf, c('credit.rating','prediction'))
caret::confusionMatrix(roundedresultsdf$credit.rating, roundedresultsdf$prediction, positive = '1')
?roc.curve
roc.curve(roundedresultsdf$credit.rating, roundedresultsdf$prediction)
par(mfrow = c(1,1))
roc.curve(roundedresultsdf$credit.rating, roundedresultsdf$prediction)
